{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../common')) # add path to common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import getdfs\n",
    "\n",
    "train_df, valid_df = getdfs(data = 'text_seq', train_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['input_str'] = train_df['input_str'].apply(lambda x : x[3:])\n",
    "valid_df['input_str'] = valid_df['input_str'].apply(lambda x : x[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = 47\n",
    "\n",
    "def get_columns (df) :\n",
    "    for i in range(num_feat):\n",
    "        df[f'c_{i}'] = df['input_str'].apply(lambda x : x[i])\n",
    "    return df.drop(columns = ['input_str'])\n",
    "\n",
    "train_df = get_columns(train_df)\n",
    "valid_df = get_columns(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_0   -0.434806\n",
      "count_1    0.378802\n",
      "count_2    0.006309\n",
      "count_3    0.002525\n",
      "count_4   -0.016974\n",
      "count_5   -0.020814\n",
      "count_6   -0.000944\n",
      "count_7    0.012974\n",
      "count_8    0.027284\n",
      "count_9   -0.000509\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = train_df\n",
    "\n",
    "# Convert string values in the 47 columns to integers\n",
    "df.iloc[:, :-1] = df.iloc[:, :-1].astype(int)  # Assuming last column is the target\n",
    "\n",
    "# Initialize a DataFrame to store counts of each number (0-9)\n",
    "count_df = pd.DataFrame(0, index=df.index, columns=[f'count_{i}' for i in range(10)])\n",
    "\n",
    "# For each number (0-9), count its occurrences across the 47 columns\n",
    "for i in range(10):\n",
    "    count_df[f'count_{i}'] = (df.iloc[:, :-1] == i).sum(axis=1)\n",
    "\n",
    "# Add the binary target column to the count_df for correlation calculation\n",
    "count_df['label'] = df['label']  # Assuming 'target' is the name of the binary target column\n",
    "\n",
    "# Calculate the Pearson correlation between the count of each number (0-9) and the target\n",
    "correlations = count_df.corr()['label'].drop('label')\n",
    "\n",
    "# Display the correlations\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>...</th>\n",
       "      <th>c_37</th>\n",
       "      <th>c_38</th>\n",
       "      <th>c_39</th>\n",
       "      <th>c_40</th>\n",
       "      <th>c_41</th>\n",
       "      <th>c_42</th>\n",
       "      <th>c_43</th>\n",
       "      <th>c_44</th>\n",
       "      <th>c_45</th>\n",
       "      <th>c_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7075</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7079</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7080 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label c_0 c_1 c_2 c_3 c_4 c_5 c_6 c_7 c_8  ... c_37 c_38 c_39 c_40 c_41  \\\n",
       "0         0   0   1   5   4   3   6   4   6   4  ...    1    5    9    6    2   \n",
       "1         0   4   6   4   1   5   9   6   3   6  ...    4    7    6    1    6   \n",
       "2         0   1   5   4   3   6   2   6   2   1  ...    0    5    1    1    5   \n",
       "3         1   0   1   5   4   3   6   4   2   2  ...    3    5    1    0    6   \n",
       "4         1   4   6   4   1   8   9   9   4   2  ...    1    5    9    6    6   \n",
       "...     ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...  ...   \n",
       "7075      1   0   0   2   6   2   6   7   0   2  ...    1    4    1    5    9   \n",
       "7076      1   0   4   6   4   4   2   2   1   5  ...    6    1    4    3    9   \n",
       "7077      1   3   0   6   9   9   1   5   9   6  ...    2    6    2    6    1   \n",
       "7078      0   0   6   1   4   1   5   9   6   1  ...    6    9    4    0    6   \n",
       "7079      0   0   1   5   4   3   6   6   1   4  ...    1    8    9    9    2   \n",
       "\n",
       "     c_42 c_43 c_44 c_45 c_46  \n",
       "0       6    2    6    1    4  \n",
       "1       1    4    2    8    4  \n",
       "2       9    6    2    8    4  \n",
       "3       1    4    2    6    2  \n",
       "4       1    4    2    8    4  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "7075    6    2    4    1    4  \n",
       "7076    0    1    2    6    2  \n",
       "7077    4    1    5    9    6  \n",
       "7078    1    4    2    8    4  \n",
       "7079    6    2    2    8    4  \n",
       "\n",
       "[7080 rows x 48 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import getdfs\n",
    "\n",
    "train_df, valid_df = getdfs(data = 'text_seq', train_size = 1)\n",
    "# train_df['input_str'] = train_df['input_str'].apply(lambda x : x[3:])\n",
    "# valid_df['input_str'] = valid_df['input_str'].apply(lambda x : x[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index,row in train_df.iterrows() :\n",
    "    # print(row['label'])\n",
    "    # a = row['input_str']\n",
    "    # i = 0\n",
    "    # while a[i] == '0' :\n",
    "    #     i += 1\n",
    "    # print(a[i:])  \n",
    "    # for i in range (8) :\n",
    "    #     print(row[f'input_str'][4*i:4*(i+1)], end = ' ')\n",
    "    # for i in range (8, 13) :\n",
    "    #     print(row[f'input_str'][32 + 3*(i-8) : 32 + 3*(i-7)], end = ' ')\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import getdfs\n",
    "\n",
    "train_df, valid_df = getdfs(data = 'text_seq', train_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs771/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>),      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ repeat_vector_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_2  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │        \u001b[38;5;34m320\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m),      │      \u001b[38;5;34m3,136\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │      \u001b[38;5;34m2,112\u001b[0m │ repeat_vector_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_2  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m)    │        \u001b[38;5;34m221\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,789</span> (22.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,789\u001b[0m (22.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,789</span> (22.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,789\u001b[0m (22.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Number of unique digits (0-9) in the input and emojis (13 unique emojis) in the output\n",
    "num_input_tokens = 10  # digits 0-9\n",
    "num_output_emojis = 13  # the number of unique emojis\n",
    "embedding_dim = 32  # Embedding size for each digit\n",
    "hidden_units = 16  # LSTM/GRU units\n",
    "\n",
    "# Model architecture\n",
    "def build_seq2seq_model(input_length=50, output_length=13):\n",
    "    # Input Layer (50-length sequence of digits 0-9)\n",
    "    encoder_inputs = layers.Input(shape=(input_length,))\n",
    "    \n",
    "    # Embedding layer (Converting digit inputs into dense vectors)\n",
    "    x = layers.Embedding(input_dim=num_input_tokens, output_dim=embedding_dim, input_length=input_length)(encoder_inputs)\n",
    "    \n",
    "    # Encoder (LSTM or GRU to encode the sequence of digits)\n",
    "    encoder_outputs, state_h, state_c = layers.LSTM(hidden_units, return_state=True)(x)\n",
    "    \n",
    "    # Repeat the encoder's context vector (state) for the number of output emojis (13)\n",
    "    decoder_inputs = layers.RepeatVector(output_length)(encoder_outputs)\n",
    "    \n",
    "    # Decoder (Using an LSTM or GRU to generate the sequence of emojis)\n",
    "    decoder_lstm = layers.LSTM(hidden_units, return_sequences=True)(decoder_inputs, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # Output Layer (Predicting emojis using a softmax layer)\n",
    "    decoder_outputs = layers.TimeDistributed(layers.Dense(num_output_emojis, activation='softmax'))(decoder_lstm)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(encoder_inputs, decoder_outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "seq2seq_model = build_seq2seq_model()\n",
    "\n",
    "# Compile the model (assuming we're using categorical crossentropy for multi-class classification)\n",
    "seq2seq_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "seq2seq_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(columns = ['label'])\n",
    "x_valid = valid_df.drop(columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = pd.read_csv('../datasets/train/train_emoticon.csv'), pd.read_csv('../datasets/valid/valid_emoticon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.drop(columns = ['label'])\n",
    "y_valid = y_valid.drop(columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1), output.shape=(None, 13, 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseq2seq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs771/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs771/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:580\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m     )\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1), output.shape=(None, 13, 13)"
     ]
    }
   ],
   "source": [
    "seq2seq_model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataframes: (You would load your actual data here)\n",
    "# Assume df_input contains a column 'digit_sequence' with 50-digit strings\n",
    "# Assume df_target contains a column 'emoji_sequence' with corresponding emoji sequences\n",
    "# Convert digits to tensor\n",
    "def digit_to_tensor(digit_str):\n",
    "    # Convert string of digits to a list of integers (0-9), then to a tensor\n",
    "    return torch.tensor([int(digit) for digit in digit_str], dtype=torch.long)\n",
    "\n",
    "# # Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis, _ = getdfs(data = 'emoticon', train_size = 1)\n",
    "emojis.drop(columns = ['label'], inplace = True)\n",
    "emoji_vocab = pd.unique(emojis.values.ravel())\n",
    "emoji_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_tensor(emoji_str):\n",
    "    # Convert string of emojis to a tensor\n",
    "    emoji_to_index = {emoji: idx for idx, emoji in enumerate(emoji_vocab)}\n",
    "    return torch.tensor([emoji_to_index[emoji] for emoji in emoji_str], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the digit sequences to tensors\n",
    "X_train = torch.stack([digit_to_tensor(seq) for seq in x_train['input_str']])\n",
    "Y_train = torch.stack([emoji_to_tensor(seq) for seq in y_train['input_emoticon']])\n",
    "X_valid = torch.stack([digit_to_tensor(seq) for seq in x_valid['input_str']])\n",
    "Y_valid = torch.stack([emoji_to_tensor(seq) for seq in y_valid['input_emoticon']])\n",
    "# convert the list of tensors to torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2780039050989904\n",
      "Epoch 2, Loss: 0.8184533242957067\n",
      "Epoch 3, Loss: 0.5288886405401311\n",
      "Epoch 4, Loss: 0.3550079667867072\n",
      "Epoch 5, Loss: 0.24353124659750505\n",
      "Epoch 6, Loss: 0.16674042834274175\n",
      "Epoch 7, Loss: 0.11576664544501221\n",
      "Epoch 8, Loss: 0.085852672954453\n",
      "Epoch 9, Loss: 0.06532602664339751\n",
      "Epoch 10, Loss: 0.0559708566974788\n",
      "Epoch 11, Loss: 0.0463611866460454\n",
      "Epoch 12, Loss: 0.040450491222379814\n",
      "Epoch 13, Loss: 0.03551777536065311\n",
      "Epoch 14, Loss: 0.03260934925541743\n",
      "Epoch 15, Loss: 0.027138370716173827\n",
      "Epoch 16, Loss: 0.026144671126412042\n",
      "Epoch 17, Loss: 0.02454997025947602\n",
      "Epoch 18, Loss: 0.020958389572360688\n",
      "Epoch 19, Loss: 0.020046057311460225\n",
      "Epoch 20, Loss: 0.019993763566041146\n",
      "Epoch 21, Loss: 0.01770787809355504\n",
      "Epoch 22, Loss: 0.017337585397340097\n",
      "Epoch 23, Loss: 0.015754676663763255\n",
      "Epoch 24, Loss: 0.01566002172282983\n",
      "Epoch 25, Loss: 0.015279213863431586\n",
      "Epoch 26, Loss: 0.014160645973309599\n",
      "Epoch 27, Loss: 0.012827800001197282\n",
      "Epoch 28, Loss: 0.012629201363020386\n",
      "Epoch 29, Loss: 0.012267364771215901\n",
      "Epoch 30, Loss: 0.012916401534927893\n",
      "Epoch 31, Loss: 0.011219871910087664\n",
      "Epoch 32, Loss: 0.011024883876000344\n",
      "Epoch 33, Loss: 0.01196335777150289\n",
      "Epoch 34, Loss: 0.009807333870787513\n",
      "Epoch 35, Loss: 0.009968554900282183\n",
      "Epoch 36, Loss: 0.010056653871788798\n",
      "Epoch 37, Loss: 0.009979138284825516\n",
      "Epoch 38, Loss: 0.009504507893635921\n",
      "Epoch 39, Loss: 0.008854664845067108\n",
      "Epoch 40, Loss: 0.007658544194832863\n",
      "Epoch 41, Loss: 0.00905003946310512\n",
      "Epoch 42, Loss: 0.007793094109109878\n",
      "Epoch 43, Loss: 0.008244986013374917\n",
      "Epoch 44, Loss: 0.007592616016251015\n",
      "Epoch 45, Loss: 0.008365040164672216\n",
      "Epoch 46, Loss: 0.007336900192250923\n",
      "Epoch 47, Loss: 0.006604036490297491\n",
      "Epoch 48, Loss: 0.00771266729319889\n",
      "Epoch 49, Loss: 0.00685643956742485\n",
      "Epoch 50, Loss: 0.0065192644215519005\n",
      "Epoch 51, Loss: 0.006435480650900107\n",
      "Epoch 52, Loss: 0.006237091686525254\n",
      "Epoch 53, Loss: 0.005979170072474851\n",
      "Epoch 54, Loss: 0.00668310720285657\n",
      "Epoch 55, Loss: 0.006382122391374545\n",
      "Epoch 56, Loss: 0.005946611565819423\n",
      "Epoch 57, Loss: 0.006136529020942171\n",
      "Epoch 58, Loss: 0.005868413976795256\n",
      "Epoch 59, Loss: 0.005778256959800053\n",
      "Epoch 60, Loss: 0.006176749339562748\n",
      "Epoch 61, Loss: 0.005016424432845233\n",
      "Epoch 62, Loss: 0.006292005032063464\n",
      "Epoch 63, Loss: 0.005004245932512549\n",
      "Epoch 64, Loss: 0.00618422819700572\n",
      "Epoch 65, Loss: 0.005330415463304705\n",
      "Epoch 66, Loss: 0.0048042436993623655\n",
      "Epoch 67, Loss: 0.005120201549436164\n",
      "Epoch 68, Loss: 0.005510654385676421\n",
      "Epoch 69, Loss: 0.0049025628978417975\n",
      "Epoch 70, Loss: 0.005291443682573071\n",
      "Epoch 71, Loss: 0.004348863912557925\n",
      "Epoch 72, Loss: 0.0049748450383517535\n",
      "Epoch 73, Loss: 0.00516507099989891\n",
      "Epoch 74, Loss: 0.004241604290780066\n",
      "Epoch 75, Loss: 0.004460243912248504\n",
      "Epoch 76, Loss: 0.005124403017848454\n",
      "Epoch 77, Loss: 0.004496185310575961\n",
      "Epoch 78, Loss: 0.0045079990197556055\n",
      "Epoch 79, Loss: 0.004399633166620196\n",
      "Epoch 80, Loss: 0.005059376920524485\n",
      "Epoch 81, Loss: 0.0042723760748860536\n",
      "Epoch 82, Loss: 0.004801692632286642\n",
      "Epoch 83, Loss: 0.004129090639572123\n",
      "Epoch 84, Loss: 0.004038447281286426\n",
      "Epoch 85, Loss: 0.003751610217505688\n",
      "Epoch 86, Loss: 0.004435184314819284\n",
      "Epoch 87, Loss: 0.004062632628926602\n",
      "Epoch 88, Loss: 0.003819248479991805\n",
      "Epoch 89, Loss: 0.0037232657950438735\n",
      "Epoch 90, Loss: 0.004012540726653538\n",
      "Epoch 91, Loss: 0.004067903076571356\n",
      "Epoch 92, Loss: 0.004255961712430238\n",
      "Epoch 93, Loss: 0.004633295774147062\n",
      "Epoch 94, Loss: 0.0032684331478432475\n",
      "Epoch 95, Loss: 0.004196918908057302\n",
      "Epoch 96, Loss: 0.004121546821498754\n",
      "Epoch 97, Loss: 0.0037604303993410827\n",
      "Epoch 98, Loss: 0.004103459644339808\n",
      "Epoch 99, Loss: 0.0039076228732367905\n",
      "Epoch 100, Loss: 0.0037831931797553723\n",
      "Test Accuracy: 74.44%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Split into train and test sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10  # 10 possible digits (0-9)\n",
    "output_dim = 216  # 13 possible emojis\n",
    "embedding_dim = 16\n",
    "hidden_dim = 128\n",
    "seq_len = 50\n",
    "emoji_seq_len = 13\n",
    "\n",
    "### Simplified Model ###\n",
    "class SimpleSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dim, seq_len, emoji_seq_len):\n",
    "        super(SimpleSeq2Seq, self).__init__()\n",
    "        \n",
    "        # Embedding layer for digit sequence\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Fully connected layers (linear transformation)\n",
    "        self.fc1 = nn.Linear(embedding_dim * seq_len, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim * emoji_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through embedding layer\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Flatten the sequence of embeddings into a single vector\n",
    "        x = x.view(x.size(0), -1)  # Shape: (batch_size, embedding_dim * seq_len)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))  # Shape: (batch_size, hidden_dim)\n",
    "        x = self.fc2(x)  # Shape: (batch_size, output_dim * emoji_seq_len)\n",
    "        \n",
    "        # Reshape the output to get emoji_seq_len sequences of output_dim emojis\n",
    "        x = x.view(x.size(0), emoji_seq_len, output_dim)  # Shape: (batch_size, emoji_seq_len, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleSeq2Seq(input_dim, output_dim, embedding_dim, hidden_dim, seq_len, emoji_seq_len)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "### Training Loop ###\n",
    "def train(model, X_train, Y_train, optimizer, criterion, epochs=100, batch_size=2):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            # Get a batch\n",
    "            x_batch = X_train[i:i + batch_size]\n",
    "            y_batch = Y_train[i:i + batch_size]\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.view(-1, output_dim)  # Shape: (batch_size * emoji_seq_len, output_dim)\n",
    "            y_batch = y_batch.view(-1)  # Shape: (batch_size * emoji_seq_len)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(X_train)}')\n",
    "\n",
    "# Train the model\n",
    "train(model, X_train, Y_train, optimizer, criterion, epochs=100)\n",
    "\n",
    "### Evaluation ###\n",
    "def evaluate(model, X_test, Y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predictions = outputs.argmax(dim=2)  # Get the most likely emoji for each position\n",
    "        accuracy = (predictions == Y_test).float().mean().item()\n",
    "        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on test set\n",
    "evaluate(model, X_valid, Y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will not work.\n",
    "we have a string of 50 digits. we know a certain substring corresponds to a certain emoji. our task is to find the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000154364642718159661428002624223132284159626...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004641596369515436422262614110471596262476161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001543626215965999614422464135806142624051159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000154364224641238614262159689561596284351061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004641899422154362069015966142624761262159661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7075</th>\n",
       "      <td>0000026267027181596614464154364222842626141596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>0000464422155826261433491543615961596284614390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>0003069915964309154366142624644222841795262614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>0000614159615436198346426242237758262159694061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7079</th>\n",
       "      <td>0000154366144221596464476126219231596614189926...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7080 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_str\n",
       "0     0000154364642718159661428002624223132284159626...\n",
       "1     0004641596369515436422262614110471596262476161...\n",
       "2     0001543626215965999614422464135806142624051159...\n",
       "3     0000154364224641238614262159689561596284351061...\n",
       "4     0004641899422154362069015966142624761262159661...\n",
       "...                                                 ...\n",
       "7075  0000026267027181596614464154364222842626141596...\n",
       "7076  0000464422155826261433491543615961596284614390...\n",
       "7077  0003069915964309154366142624644222841795262614...\n",
       "7078  0000614159615436198346426242237758262159694061...\n",
       "7079  0000154366144221596464476126219231596614189926...\n",
       "\n",
       "[7080 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs771",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
